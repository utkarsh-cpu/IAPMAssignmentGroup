{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Portfolio Optimization\n",
    "\n",
    "**Addressing Limitations of Classical Mean-Variance Optimization (MVO)**\n",
    "\n",
    "This notebook implements modern portfolio optimization techniques that address the well-documented theoretical limitations of classical MVO:\n",
    "\n",
    "1. **Black-Litterman Model** - Fixes the \"Estimation Error Maximizer\" problem\n",
    "2. **Ledoit-Wolf Shrinkage Estimator** - Fixes sample covariance matrix instability  \n",
    "3. **EWMA/Dynamic Volatility** - Provides forward-looking risk estimates\n",
    "4. **Downside Risk Optimization (CVaR/Sortino)** - Handles non-normal return distributions\n",
    "5. **Hierarchical Risk Parity (HRP)** - Machine learning-based portfolio construction\n",
    "6. **Walk-Forward Optimization** - Proper out-of-sample backtesting\n",
    "\n",
    "---\n",
    "\n",
    "*IAPM Portfolio Project - Bharath, Chelsea, Gaurav, Vikram, Utkarsh, Yash*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# IMPORTS AND CONFIGURATION\n# ============================================================\n\nimport os\nimport io\nimport zipfile\nimport warnings\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport glob\nfrom typing import Optional, Dict, List, Tuple, Union\nfrom scipy.optimize import minimize\nfrom scipy.cluster.hierarchy import linkage, leaves_list, dendrogram\nfrom scipy.spatial.distance import squareform\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# Configuration Constants\nTRADING_DAYS = 252\nRISK_FREE_RATE = 0.06 / TRADING_DAYS  # Daily risk-free rate (6% annual / 252 trading days)\n\nprint(\"Advanced Portfolio Optimization - Setup Complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Utilities\n",
    "\n",
    "Functions to load and prepare stock data from the existing data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# DATA LOADING UTILITIES\n# ============================================================\n\ndef find_data_root(base_folder: str) -> Optional[str]:\n    \"\"\"\n    Recursively search for Companies_list.csv to detect valid data root.\n    \"\"\"\n    for root, dirs, files in os.walk(base_folder):\n        if \"Companies_list.csv\" in files and \"HISTORICAL_DATA\" in dirs:\n            return root\n    return None\n\n\ndef load_company_csv_strict(path: str, symbol: str) -> Optional[pd.DataFrame]:\n    \"\"\"\n    Load single CSV only if it contains 'Date' and a close-like column.\n    \"\"\"\n    try:\n        raw = pd.read_csv(path, low_memory=False)\n        raw.columns = [str(c).strip() for c in raw.columns]\n        date_col = next((c for c in raw.columns if c.lower() == \"date\"), None)\n        if date_col is None:\n            return None\n        raw[date_col] = pd.to_datetime(raw[date_col], errors=\"coerce\")\n        raw = raw.dropna(subset=[date_col])\n        close_candidates = [c for c in raw.columns if c.lower() in \n                          (\"adj_close\", \"adj close\", \"close\", \"close_price\", \"close price\")]\n        if not close_candidates:\n            return None\n        close_col = close_candidates[0]\n        df = raw[[date_col, close_col]].rename(columns={date_col: \"Date\", close_col: symbol})\n        df = df.dropna().drop_duplicates(subset=[\"Date\"]).sort_values(\"Date\").set_index(\"Date\")\n        return df\n    except Exception:\n        return None\n\n\ndef read_stock_data_from_folder(folder_path: str) -> pd.DataFrame:\n    \"\"\"\n    Reads stock dataset from folder structure with Companies_list.csv\n    and HISTORICAL_DATA/*.csv files.\n    \n    Returns DataFrame with date index and stock symbols as columns.\n    \"\"\"\n    companies_csv = os.path.join(folder_path, \"Companies_list.csv\")\n    hist_dir = os.path.join(folder_path, \"HISTORICAL_DATA\")\n    \n    if not os.path.exists(companies_csv):\n        raise Exception(\"Companies_list.csv not found\")\n    if not os.path.exists(hist_dir):\n        raise Exception(\"HISTORICAL_DATA folder not found\")\n    \n    companies_df = pd.read_csv(companies_csv)\n    companies_df.columns = [c.strip() for c in companies_df.columns]\n    symbol_col = next(\n        (c for c in companies_df.columns\n         if c.lower() in (\"symbol\", \"ticker\", \"company\", \"stock\", \"code\")),\n        companies_df.columns[0]\n    )\n    symbols = companies_df[symbol_col].astype(str).tolist()\n    \n    files = glob.glob(os.path.join(hist_dir, \"*_data.csv\"))\n    file_map = {}\n    for f in files:\n        name = os.path.basename(f)\n        prefix = name.replace(\"_data.csv\", \"\")\n        file_map[prefix.upper()] = f\n    \n    frames = []\n    loaded_symbols = []\n    for sym in symbols:\n        path = file_map.get(sym.upper())\n        if path:\n            df = load_company_csv_strict(path, sym)\n            if df is not None:\n                frames.append(df)\n                loaded_symbols.append(sym)\n    \n    if not frames:\n        raise Exception(\"No valid stock data found\")\n    \n    combined = pd.concat(frames, axis=1)\n    combined = combined.dropna(how='all')\n    combined = combined.reset_index().rename(columns={'index': 'date', 'Date': 'date'})\n    \n    return combined\n\n\ndef filter_dataset(\n    df: pd.DataFrame,\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    company_codes: Optional[List[str]] = None\n) -> pd.DataFrame:\n    \"\"\"\n    Filters DataFrame by date range and/or company codes.\n    \"\"\"\n    filtered = df.copy()\n    filtered['date'] = pd.to_datetime(filtered['date'])\n    \n    if start_date:\n        filtered = filtered[filtered['date'] >= pd.to_datetime(start_date)]\n    if end_date:\n        filtered = filtered[filtered['date'] <= pd.to_datetime(end_date)]\n    if company_codes:\n        cols = ['date'] + [c for c in company_codes if c in filtered.columns]\n        filtered = filtered[cols]\n    \n    return filtered.dropna()\n\n\ndef compute_returns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes daily returns from price DataFrame.\n    \"\"\"\n    price_cols = [c for c in df.columns if c != 'date']\n    prices = df[price_cols].copy()\n    returns = prices.pct_change().dropna()\n    return returns\n\n\ndef generate_sample_data(\n    n_assets: int = 10,\n    n_days: int = 500,\n    seed: int = 42\n) -> pd.DataFrame:\n    \"\"\"\n    Generates synthetic stock price data for demonstration.\n    \n    Parameters\n    ----------\n    n_assets : int\n        Number of synthetic stocks to generate.\n    n_days : int\n        Number of trading days.\n    seed : int\n        Random seed for reproducibility.\n    \n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with 'date' column and stock price columns.\n    \"\"\"\n    np.random.seed(seed)\n    \n    # Create date range\n    dates = pd.date_range(start='2020-01-01', periods=n_days, freq='B')\n    \n    # Generate correlated returns using Cholesky decomposition\n    # Create a random correlation matrix\n    A = np.random.randn(n_assets, n_assets)\n    corr_matrix = A @ A.T\n    d = np.sqrt(np.diag(corr_matrix))\n    corr_matrix = corr_matrix / np.outer(d, d)\n    \n    # Generate daily volatilities (1.5-3.5% daily std, roughly 24-56% annualized)\n    daily_vols = np.random.uniform(0.015, 0.035, n_assets)\n    \n    # Generate daily expected returns (0.02-0.06% daily, roughly 5-15% annualized)\n    daily_means = np.random.uniform(0.0002, 0.0006, n_assets)\n    \n    # Generate correlated returns\n    L = np.linalg.cholesky(corr_matrix)\n    uncorrelated = np.random.randn(n_days, n_assets)\n    correlated = uncorrelated @ L.T\n    returns = correlated * daily_vols + daily_means\n    \n    # Convert to prices\n    prices = np.zeros((n_days, n_assets))\n    prices[0] = 100  # Initial price of $100\n    for t in range(1, n_days):\n        prices[t] = prices[t-1] * (1 + returns[t])\n    \n    # Create DataFrame\n    symbols = [f'STOCK_{i+1:02d}' for i in range(n_assets)]\n    df = pd.DataFrame(prices, columns=symbols)\n    df['date'] = dates\n    df = df[['date'] + symbols]\n    \n    return df\n\n\nprint(\"Data loading utilities defined.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Black-Litterman Model\n",
    "\n",
    "### The Problem: \"Estimation Error Maximizer\"\n",
    "\n",
    "Classical MVO uses historical mean returns to estimate future expected returns. This approach:\n",
    "- Heavily overweights assets with abnormally high historical returns\n",
    "- Underweights assets with poor past performance\n",
    "- Blindly assumes the past perfectly dictates the future\n",
    "\n",
    "### The Solution: Black-Litterman Model\n",
    "\n",
    "The Black-Litterman model:\n",
    "1. Starts with market equilibrium returns implied by market cap weights\n",
    "2. Allows blending with investor views (absolute or relative forecasts)\n",
    "3. Produces more stable and intuitive portfolio weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLACK-LITTERMAN MODEL\n",
    "# ============================================================\n",
    "\n",
    "def compute_implied_equilibrium_returns(\n",
    "    Sigma: np.ndarray,\n",
    "    market_weights: np.ndarray,\n",
    "    risk_aversion: float = 2.5,\n",
    "    risk_free_rate: float = 0.0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute implied equilibrium returns using reverse optimization.\n",
    "    \n",
    "    The market portfolio is assumed to be optimal, so we back out\n",
    "    what expected returns would make these weights optimal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Sigma : np.ndarray\n",
    "        Covariance matrix of returns (n x n).\n",
    "    market_weights : np.ndarray\n",
    "        Market capitalization weights (n,).\n",
    "    risk_aversion : float\n",
    "        Risk aversion parameter (lambda). Higher = more risk averse.\n",
    "    risk_free_rate : float\n",
    "        Risk-free rate.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Implied equilibrium expected returns (n,).\n",
    "    \"\"\"\n",
    "    # Pi = lambda * Sigma * w_mkt\n",
    "    pi = risk_aversion * (Sigma @ market_weights)\n",
    "    return pi\n",
    "\n",
    "\n",
    "def black_litterman_posterior(\n",
    "    Sigma: np.ndarray,\n",
    "    equilibrium_returns: np.ndarray,\n",
    "    P: Optional[np.ndarray] = None,\n",
    "    Q: Optional[np.ndarray] = None,\n",
    "    omega: Optional[np.ndarray] = None,\n",
    "    tau: float = 0.05\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute Black-Litterman posterior expected returns and covariance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Sigma : np.ndarray\n",
    "        Covariance matrix of returns (n x n).\n",
    "    equilibrium_returns : np.ndarray\n",
    "        Implied equilibrium returns from market cap weights (n,).\n",
    "    P : np.ndarray, optional\n",
    "        View matrix (k x n) where k is number of views.\n",
    "        Each row represents one view on assets.\n",
    "    Q : np.ndarray, optional\n",
    "        View returns (k,). Expected returns for each view.\n",
    "    omega : np.ndarray, optional\n",
    "        View uncertainty matrix (k x k). Confidence in each view.\n",
    "        If None, uses proportional to implied variance.\n",
    "    tau : float\n",
    "        Scaling parameter for prior uncertainty. Typically 0.01-0.05.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, np.ndarray]\n",
    "        (posterior_returns, posterior_covariance)\n",
    "    \"\"\"\n",
    "    n = len(equilibrium_returns)\n",
    "    tau_Sigma = tau * Sigma\n",
    "    \n",
    "    # If no views provided, return equilibrium\n",
    "    if P is None or Q is None:\n",
    "        return equilibrium_returns, Sigma + tau_Sigma\n",
    "    \n",
    "    P = np.atleast_2d(P)\n",
    "    Q = np.atleast_1d(Q)\n",
    "    k = P.shape[0]\n",
    "    \n",
    "    # Default omega: proportional to view portfolio variance\n",
    "    if omega is None:\n",
    "        omega = np.diag(np.diag(P @ tau_Sigma @ P.T))\n",
    "    \n",
    "    # Black-Litterman formula for posterior mean\n",
    "    # E[R] = [(tau*Sigma)^-1 + P'*Omega^-1*P]^-1 * [(tau*Sigma)^-1*Pi + P'*Omega^-1*Q]\n",
    "    tau_Sigma_inv = np.linalg.inv(tau_Sigma)\n",
    "    omega_inv = np.linalg.inv(omega)\n",
    "    \n",
    "    M = tau_Sigma_inv + P.T @ omega_inv @ P\n",
    "    posterior_returns = np.linalg.solve(M, tau_Sigma_inv @ equilibrium_returns + P.T @ omega_inv @ Q)\n",
    "    \n",
    "    # Posterior covariance\n",
    "    posterior_cov = Sigma + np.linalg.inv(M)\n",
    "    \n",
    "    return posterior_returns, posterior_cov\n",
    "\n",
    "\n",
    "def black_litterman_portfolio(\n",
    "    returns_df: pd.DataFrame,\n",
    "    market_caps: Optional[np.ndarray] = None,\n",
    "    views: Optional[List[Dict]] = None,\n",
    "    risk_aversion: float = 2.5,\n",
    "    tau: float = 0.05,\n",
    "    risk_free_rate: float = 0.0\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Complete Black-Litterman portfolio optimization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns_df : pd.DataFrame\n",
    "        Returns DataFrame with assets as columns.\n",
    "    market_caps : np.ndarray, optional\n",
    "        Market capitalizations. If None, uses equal weights.\n",
    "    views : List[Dict], optional\n",
    "        List of views. Each view is a dict with:\n",
    "        - 'assets': list of asset names or single asset\n",
    "        - 'return': expected return\n",
    "        - 'type': 'absolute' or 'relative'\n",
    "        - 'confidence': optional confidence level (default 1.0)\n",
    "    risk_aversion : float\n",
    "        Risk aversion parameter.\n",
    "    tau : float\n",
    "        Prior uncertainty parameter.\n",
    "    risk_free_rate : float\n",
    "        Risk-free rate.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        Portfolio results including weights, expected returns, risk.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> views = [\n",
    "    ...     {'assets': 'AAPL', 'return': 0.10, 'type': 'absolute'},\n",
    "    ...     {'assets': ['GOOG', 'MSFT'], 'return': 0.02, 'type': 'relative'},  # GOOG outperforms MSFT by 2%\n",
    "    ... ]\n",
    "    \"\"\"\n",
    "    returns = returns_df.values\n",
    "    n = returns.shape[1]\n",
    "    asset_names = list(returns_df.columns)\n",
    "    \n",
    "    # Compute sample covariance\n",
    "    Sigma = np.cov(returns, rowvar=False)\n",
    "    \n",
    "    # Market weights (equal if not provided)\n",
    "    if market_caps is None:\n",
    "        market_weights = np.ones(n) / n\n",
    "    else:\n",
    "        market_weights = market_caps / market_caps.sum()\n",
    "    \n",
    "    # Implied equilibrium returns\n",
    "    pi = compute_implied_equilibrium_returns(\n",
    "        Sigma, market_weights, risk_aversion, risk_free_rate\n",
    "    )\n",
    "    \n",
    "    # Process views into P and Q matrices\n",
    "    P = None\n",
    "    Q = None\n",
    "    omega = None\n",
    "    \n",
    "    if views:\n",
    "        P_list = []\n",
    "        Q_list = []\n",
    "        conf_list = []\n",
    "        \n",
    "        for view in views:\n",
    "            assets = view['assets']\n",
    "            ret = view['return']\n",
    "            view_type = view.get('type', 'absolute')\n",
    "            confidence = view.get('confidence', 1.0)\n",
    "            \n",
    "            p_row = np.zeros(n)\n",
    "            \n",
    "            if view_type == 'absolute':\n",
    "                if isinstance(assets, str):\n",
    "                    assets = [assets]\n",
    "                for asset in assets:\n",
    "                    if asset in asset_names:\n",
    "                        idx = asset_names.index(asset)\n",
    "                        p_row[idx] = 1.0 / len(assets)\n",
    "            elif view_type == 'relative':\n",
    "                # First asset outperforms second by 'return'\n",
    "                if len(assets) >= 2:\n",
    "                    idx1 = asset_names.index(assets[0]) if assets[0] in asset_names else -1\n",
    "                    idx2 = asset_names.index(assets[1]) if assets[1] in asset_names else -1\n",
    "                    if idx1 >= 0 and idx2 >= 0:\n",
    "                        p_row[idx1] = 1.0\n",
    "                        p_row[idx2] = -1.0\n",
    "            \n",
    "            if np.any(p_row != 0):\n",
    "                P_list.append(p_row)\n",
    "                Q_list.append(ret)\n",
    "                conf_list.append(confidence)\n",
    "        \n",
    "        if P_list:\n",
    "            P = np.array(P_list)\n",
    "            Q = np.array(Q_list)\n",
    "            # Omega scaled by confidence\n",
    "            tau_Sigma = tau * Sigma\n",
    "            view_vars = np.diag(P @ tau_Sigma @ P.T)\n",
    "            omega = np.diag(view_vars / np.array(conf_list))\n",
    "    \n",
    "    # Compute posterior\n",
    "    posterior_returns, posterior_cov = black_litterman_posterior(\n",
    "        Sigma, pi, P, Q, omega, tau\n",
    "    )\n",
    "    \n",
    "    # Optimize portfolio using posterior estimates\n",
    "    # Mean-variance optimization with posterior estimates\n",
    "    def neg_sharpe(w):\n",
    "        ret = w @ posterior_returns\n",
    "        risk = np.sqrt(w @ posterior_cov @ w)\n",
    "        return -(ret - risk_free_rate) / risk if risk > 0 else 0\n",
    "    \n",
    "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "    bounds = [(0, 1) for _ in range(n)]\n",
    "    w0 = np.ones(n) / n\n",
    "    \n",
    "    result = minimize(neg_sharpe, w0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    weights = result.x\n",
    "    \n",
    "    # Compute portfolio metrics\n",
    "    port_return = weights @ posterior_returns\n",
    "    port_risk = np.sqrt(weights @ posterior_cov @ weights)\n",
    "    sharpe = (port_return - risk_free_rate) / port_risk if port_risk > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'weights': weights,\n",
    "        'asset_names': asset_names,\n",
    "        'return': port_return,\n",
    "        'risk': port_risk,\n",
    "        'sharpe': sharpe,\n",
    "        'equilibrium_returns': pi,\n",
    "        'posterior_returns': posterior_returns,\n",
    "        'posterior_covariance': posterior_cov,\n",
    "        'market_weights': market_weights\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Black-Litterman model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Ledoit-Wolf Shrinkage Estimator\n",
    "\n",
    "### The Problem: Sample Covariance Instability\n",
    "\n",
    "The standard sample covariance matrix (`np.cov()`):\n",
    "- Becomes ill-conditioned when N (assets) is large relative to T (time periods)\n",
    "- The optimizer exploits spurious correlations\n",
    "- Results in extreme, unstable portfolio weights\n",
    "\n",
    "### The Solution: Ledoit-Wolf Shrinkage\n",
    "\n",
    "Shrinkage \"pulls\" the empirical covariance toward a structured target (constant correlation model), reducing the impact of extreme, likely false, covariance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LEDOIT-WOLF SHRINKAGE ESTIMATOR\n",
    "# ============================================================\n",
    "\n",
    "def ledoit_wolf_shrinkage(\n",
    "    returns: np.ndarray,\n",
    "    shrinkage_target: str = 'constant_correlation'\n",
    ") -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Compute Ledoit-Wolf shrinkage covariance estimator.\n",
    "    \n",
    "    The shrinkage estimator combines the sample covariance matrix with\n",
    "    a structured target to produce a more stable estimate.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : np.ndarray\n",
    "        T x N matrix of returns.\n",
    "    shrinkage_target : str\n",
    "        'constant_correlation' or 'identity'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, float]\n",
    "        (shrunk_covariance, shrinkage_intensity)\n",
    "    \"\"\"\n",
    "    T, N = returns.shape\n",
    "    \n",
    "    # De-mean returns\n",
    "    returns_centered = returns - returns.mean(axis=0)\n",
    "    \n",
    "    # Sample covariance\n",
    "    sample_cov = (returns_centered.T @ returns_centered) / T\n",
    "    \n",
    "    # Sample variances and standard deviations\n",
    "    sample_var = np.diag(sample_cov)\n",
    "    sample_std = np.sqrt(sample_var)\n",
    "    \n",
    "    # Compute shrinkage target\n",
    "    if shrinkage_target == 'constant_correlation':\n",
    "        # Constant correlation model: same average correlation everywhere\n",
    "        std_outer = np.outer(sample_std, sample_std)\n",
    "        sample_corr = sample_cov / np.where(std_outer > 0, std_outer, 1)\n",
    "        np.fill_diagonal(sample_corr, 1.0)\n",
    "        \n",
    "        # Average off-diagonal correlation\n",
    "        off_diag_mask = ~np.eye(N, dtype=bool)\n",
    "        avg_corr = np.mean(sample_corr[off_diag_mask])\n",
    "        \n",
    "        # Target: constant correlation matrix scaled by variances\n",
    "        F = avg_corr * std_outer\n",
    "        np.fill_diagonal(F, sample_var)\n",
    "    else:\n",
    "        # Identity (scaled by average variance)\n",
    "        avg_var = np.mean(sample_var)\n",
    "        F = avg_var * np.eye(N)\n",
    "    \n",
    "    # Compute optimal shrinkage intensity using Ledoit-Wolf formula\n",
    "    # Simplified version - computes sample quantities\n",
    "    \n",
    "    # pi: sum of asymptotic variances of scaled sample covariances\n",
    "    X = returns_centered\n",
    "    pi_sum = 0.0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            term = X[:, i] * X[:, j] - sample_cov[i, j]\n",
    "            pi_sum += np.sum(term ** 2) / T\n",
    "    \n",
    "    # gamma: misspecification (Frobenius norm of S - F)\n",
    "    gamma = np.sum((sample_cov - F) ** 2)\n",
    "    \n",
    "    # Optimal shrinkage intensity\n",
    "    kappa = (pi_sum - gamma) / gamma if gamma > 0 else 0\n",
    "    shrinkage_intensity = max(0, min(1, kappa / T))\n",
    "    \n",
    "    # Shrunk covariance\n",
    "    shrunk_cov = shrinkage_intensity * F + (1 - shrinkage_intensity) * sample_cov\n",
    "    \n",
    "    return shrunk_cov, shrinkage_intensity\n",
    "\n",
    "\n",
    "def portfolio_optimizer_shrinkage(\n",
    "    returns_df: pd.DataFrame,\n",
    "    risk_free_rate: float = 0.0,\n",
    "    shrinkage_target: str = 'constant_correlation'\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Portfolio optimization using Ledoit-Wolf shrinkage covariance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns_df : pd.DataFrame\n",
    "        Returns DataFrame.\n",
    "    risk_free_rate : float\n",
    "        Risk-free rate.\n",
    "    shrinkage_target : str\n",
    "        Shrinkage target type.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        Portfolio results.\n",
    "    \"\"\"\n",
    "    returns = returns_df.values\n",
    "    n = returns.shape[1]\n",
    "    \n",
    "    # Mean returns\n",
    "    mu = np.mean(returns, axis=0)\n",
    "    \n",
    "    # Shrunk covariance\n",
    "    Sigma, shrinkage_intensity = ledoit_wolf_shrinkage(returns, shrinkage_target)\n",
    "    \n",
    "    # GMVP\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    ones = np.ones(n)\n",
    "    w_gmvp = Sigma_inv @ ones / (ones @ Sigma_inv @ ones)\n",
    "    w_gmvp = np.maximum(w_gmvp, 0)\n",
    "    w_gmvp /= w_gmvp.sum()\n",
    "    \n",
    "    # MRR (Max Sharpe)\n",
    "    excess = mu - risk_free_rate\n",
    "    w_mrr = Sigma_inv @ excess\n",
    "    w_mrr = np.maximum(w_mrr, 0)\n",
    "    if w_mrr.sum() > 0:\n",
    "        w_mrr /= w_mrr.sum()\n",
    "    else:\n",
    "        w_mrr = np.ones(n) / n\n",
    "    \n",
    "    # Compute metrics\n",
    "    gmvp_ret = w_gmvp @ mu\n",
    "    gmvp_risk = np.sqrt(w_gmvp @ Sigma @ w_gmvp)\n",
    "    \n",
    "    mrr_ret = w_mrr @ mu\n",
    "    mrr_risk = np.sqrt(w_mrr @ Sigma @ w_mrr)\n",
    "    mrr_sharpe = (mrr_ret - risk_free_rate) / mrr_risk if mrr_risk > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'gmvp': {'weights': w_gmvp, 'return': gmvp_ret, 'risk': gmvp_risk},\n",
    "        'mrr': {'weights': w_mrr, 'return': mrr_ret, 'risk': mrr_risk, 'sharpe': mrr_sharpe},\n",
    "        'shrinkage_intensity': shrinkage_intensity,\n",
    "        'shrunk_covariance': Sigma,\n",
    "        'mean_returns': mu\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Ledoit-Wolf shrinkage estimator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Dynamic Volatility (EWMA)\n",
    "\n",
    "### The Problem: Static Covariance Assumptions\n",
    "\n",
    "Returns are heteroskedastic (volatility clusters over time). A simple equally-weighted historical covariance matrix doesn't capture recent volatility changes.\n",
    "\n",
    "### The Solution: Exponentially Weighted Moving Average (EWMA)\n",
    "\n",
    "EWMA gives more weight to recent observations, providing a more forward-looking risk estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DYNAMIC VOLATILITY (EWMA)\n",
    "# ============================================================\n",
    "\n",
    "def ewma_covariance(\n",
    "    returns: np.ndarray,\n",
    "    lambda_decay: float = 0.94\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute EWMA (Exponentially Weighted Moving Average) covariance matrix.\n",
    "    \n",
    "    More recent observations receive higher weight, making this\n",
    "    a more responsive estimate of current market conditions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : np.ndarray\n",
    "        T x N matrix of returns.\n",
    "    lambda_decay : float\n",
    "        Decay factor (0 < lambda < 1). Higher = more weight to older obs.\n",
    "        RiskMetrics uses 0.94 for daily data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        EWMA covariance matrix (N x N).\n",
    "    \"\"\"\n",
    "    T, N = returns.shape\n",
    "    \n",
    "    # De-mean returns\n",
    "    returns_centered = returns - returns.mean(axis=0)\n",
    "    \n",
    "    # Initialize with sample covariance\n",
    "    cov = np.zeros((N, N))\n",
    "    \n",
    "    # Compute EWMA iteratively\n",
    "    for t in range(T):\n",
    "        r = returns_centered[t:t+1].T  # Column vector\n",
    "        if t == 0:\n",
    "            cov = r @ r.T\n",
    "        else:\n",
    "            cov = lambda_decay * cov + (1 - lambda_decay) * (r @ r.T)\n",
    "    \n",
    "    return cov\n",
    "\n",
    "\n",
    "def portfolio_optimizer_ewma(\n",
    "    returns_df: pd.DataFrame,\n",
    "    risk_free_rate: float = 0.0,\n",
    "    lambda_decay: float = 0.94\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Portfolio optimization using EWMA covariance matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns_df : pd.DataFrame\n",
    "        Returns DataFrame.\n",
    "    risk_free_rate : float\n",
    "        Risk-free rate.\n",
    "    lambda_decay : float\n",
    "        EWMA decay factor.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        Portfolio results.\n",
    "    \"\"\"\n",
    "    returns = returns_df.values\n",
    "    n = returns.shape[1]\n",
    "    \n",
    "    # Mean returns (can also use EWMA mean if desired)\n",
    "    mu = np.mean(returns, axis=0)\n",
    "    \n",
    "    # EWMA covariance\n",
    "    Sigma = ewma_covariance(returns, lambda_decay)\n",
    "    \n",
    "    # Ensure positive definiteness\n",
    "    eigvals = np.linalg.eigvalsh(Sigma)\n",
    "    if eigvals.min() <= 0:\n",
    "        # Add small regularization\n",
    "        Sigma += np.eye(n) * abs(eigvals.min()) * 1.1\n",
    "    \n",
    "    # GMVP\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    ones = np.ones(n)\n",
    "    w_gmvp = Sigma_inv @ ones / (ones @ Sigma_inv @ ones)\n",
    "    w_gmvp = np.maximum(w_gmvp, 0)\n",
    "    w_gmvp /= w_gmvp.sum()\n",
    "    \n",
    "    # MRR\n",
    "    excess = mu - risk_free_rate\n",
    "    w_mrr = Sigma_inv @ excess\n",
    "    w_mrr = np.maximum(w_mrr, 0)\n",
    "    if w_mrr.sum() > 0:\n",
    "        w_mrr /= w_mrr.sum()\n",
    "    else:\n",
    "        w_mrr = np.ones(n) / n\n",
    "    \n",
    "    # Metrics\n",
    "    gmvp_ret = w_gmvp @ mu\n",
    "    gmvp_risk = np.sqrt(w_gmvp @ Sigma @ w_gmvp)\n",
    "    \n",
    "    mrr_ret = w_mrr @ mu\n",
    "    mrr_risk = np.sqrt(w_mrr @ Sigma @ w_mrr)\n",
    "    mrr_sharpe = (mrr_ret - risk_free_rate) / mrr_risk if mrr_risk > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'gmvp': {'weights': w_gmvp, 'return': gmvp_ret, 'risk': gmvp_risk},\n",
    "        'mrr': {'weights': w_mrr, 'return': mrr_ret, 'risk': mrr_risk, 'sharpe': mrr_sharpe},\n",
    "        'ewma_covariance': Sigma,\n",
    "        'lambda_decay': lambda_decay\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Dynamic volatility (EWMA) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Downside Risk Optimization (CVaR / Sortino)\n",
    "\n",
    "### The Problem: Normal Distribution Assumptions\n",
    "\n",
    "MVO defines risk as variance (\u03c3\u00b2), assuming normal distributions. In reality:\n",
    "- Stock returns exhibit \"fat tails\" (kurtosis)\n",
    "- Negative skewness means extreme losses happen more often than predicted\n",
    "\n",
    "### The Solution: Downside Risk Measures\n",
    "\n",
    "- **CVaR (Conditional Value at Risk)**: Expected loss given we're in the tail\n",
    "- **Sortino Ratio**: Like Sharpe, but only penalizes downside volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DOWNSIDE RISK OPTIMIZATION (CVaR / SORTINO)\n",
    "# ============================================================\n",
    "\n",
    "def compute_var_cvar(\n",
    "    returns: np.ndarray,\n",
    "    weights: np.ndarray,\n",
    "    confidence: float = 0.95\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute Value at Risk (VaR) and Conditional VaR (Expected Shortfall).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : np.ndarray\n",
    "        T x N matrix of returns.\n",
    "    weights : np.ndarray\n",
    "        Portfolio weights (N,).\n",
    "    confidence : float\n",
    "        Confidence level (e.g., 0.95 for 95% VaR).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float]\n",
    "        (VaR, CVaR) as positive numbers representing loss.\n",
    "    \"\"\"\n",
    "    # Portfolio returns\n",
    "    port_returns = returns @ weights\n",
    "    \n",
    "    # VaR: quantile of the loss distribution\n",
    "    alpha = 1 - confidence\n",
    "    var = -np.percentile(port_returns, alpha * 100)\n",
    "    \n",
    "    # CVaR: expected loss given we're beyond VaR\n",
    "    losses = -port_returns\n",
    "    cvar = np.mean(losses[losses >= var])\n",
    "    \n",
    "    return var, cvar\n",
    "\n",
    "\n",
    "def compute_sortino_ratio(\n",
    "    returns: np.ndarray,\n",
    "    weights: np.ndarray,\n",
    "    risk_free_rate: float = 0.0,\n",
    "    target_return: float = 0.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute Sortino ratio (return over downside deviation).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : np.ndarray\n",
    "        T x N matrix of returns.\n",
    "    weights : np.ndarray\n",
    "        Portfolio weights.\n",
    "    risk_free_rate : float\n",
    "        Risk-free rate.\n",
    "    target_return : float\n",
    "        Minimum acceptable return (MAR).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Sortino ratio.\n",
    "    \"\"\"\n",
    "    port_returns = returns @ weights\n",
    "    excess_return = np.mean(port_returns) - risk_free_rate\n",
    "    \n",
    "    # Downside deviation\n",
    "    downside_returns = np.minimum(port_returns - target_return, 0)\n",
    "    downside_std = np.sqrt(np.mean(downside_returns ** 2))\n",
    "    \n",
    "    if downside_std > 0:\n",
    "        return excess_return / downside_std\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def min_cvar_portfolio(\n",
    "    returns_df: pd.DataFrame,\n",
    "    confidence: float = 0.95,\n",
    "    target_return: Optional[float] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Optimize portfolio to minimize CVaR (Conditional Value at Risk).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns_df : pd.DataFrame\n",
    "        Returns DataFrame.\n",
    "    confidence : float\n",
    "        Confidence level.\n",
    "    target_return : float, optional\n",
    "        Target portfolio return constraint.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        Portfolio results.\n",
    "    \"\"\"\n",
    "    returns = returns_df.values\n",
    "    T, n = returns.shape\n",
    "    \n",
    "    def cvar_objective(w):\n",
    "        _, cvar = compute_var_cvar(returns, w, confidence)\n",
    "        return cvar\n",
    "    \n",
    "    constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n",
    "    if target_return is not None:\n",
    "        mu = np.mean(returns, axis=0)\n",
    "        constraints.append({'type': 'eq', 'fun': lambda w: w @ mu - target_return})\n",
    "    \n",
    "    bounds = [(0, 1) for _ in range(n)]\n",
    "    w0 = np.ones(n) / n\n",
    "    \n",
    "    result = minimize(\n",
    "        cvar_objective, w0, method='SLSQP',\n",
    "        bounds=bounds, constraints=constraints,\n",
    "        options={'ftol': 1e-8, 'maxiter': 500}\n",
    "    )\n",
    "    \n",
    "    weights = result.x\n",
    "    port_returns = returns @ weights\n",
    "    var, cvar = compute_var_cvar(returns, weights, confidence)\n",
    "    \n",
    "    return {\n",
    "        'weights': weights,\n",
    "        'return': np.mean(port_returns),\n",
    "        'risk': np.std(port_returns),\n",
    "        'var': var,\n",
    "        'cvar': cvar,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "\n",
    "def max_sortino_portfolio(\n",
    "    returns_df: pd.DataFrame,\n",
    "    risk_free_rate: float = 0.0,\n",
    "    target_return: float = 0.0\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Optimize portfolio to maximize Sortino ratio.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns_df : pd.DataFrame\n",
    "        Returns DataFrame.\n",
    "    risk_free_rate : float\n",
    "        Risk-free rate.\n",
    "    target_return : float\n",
    "        Minimum acceptable return.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        Portfolio results.\n",
    "    \"\"\"\n",
    "    returns = returns_df.values\n",
    "    n = returns.shape[1]\n",
    "    \n",
    "    def neg_sortino(w):\n",
    "        return -compute_sortino_ratio(returns, w, risk_free_rate, target_return)\n",
    "    \n",
    "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "    bounds = [(0, 1) for _ in range(n)]\n",
    "    w0 = np.ones(n) / n\n",
    "    \n",
    "    result = minimize(\n",
    "        neg_sortino, w0, method='SLSQP',\n",
    "        bounds=bounds, constraints=constraints,\n",
    "        options={'ftol': 1e-8, 'maxiter': 500}\n",
    "    )\n",
    "    \n",
    "    weights = result.x\n",
    "    port_returns = returns @ weights\n",
    "    sortino = compute_sortino_ratio(returns, weights, risk_free_rate, target_return)\n",
    "    \n",
    "    return {\n",
    "        'weights': weights,\n",
    "        'return': np.mean(port_returns),\n",
    "        'risk': np.std(port_returns),\n",
    "        'sortino_ratio': sortino,\n",
    "        'sharpe_ratio': (np.mean(port_returns) - risk_free_rate) / np.std(port_returns)\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Downside risk optimization (CVaR/Sortino) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Hierarchical Risk Parity (HRP)\n",
    "\n",
    "### The Problem: Traditional Optimization Instability\n",
    "\n",
    "Standard MVO requires inverting the covariance matrix, which:\n",
    "- Can be numerically unstable\n",
    "- Doesn't account for hierarchical structure in markets\n",
    "- Requires return predictions\n",
    "\n",
    "### The Solution: Hierarchical Risk Parity (HRP)\n",
    "\n",
    "Developed by Marcos Lopez de Prado, HRP:\n",
    "1. Uses agglomerative hierarchical clustering on correlation distance\n",
    "2. Groups stocks based on similarity\n",
    "3. Allocates risk equally across clusters\n",
    "4. **Avoids covariance matrix inversion entirely**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HIERARCHICAL RISK PARITY (HRP)\n",
    "# ============================================================\n",
    "\n",
    "def correlation_distance(corr_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert correlation matrix to distance matrix.\n",
    "    \n",
    "    Distance = sqrt(0.5 * (1 - correlation))\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corr_matrix : np.ndarray\n",
    "        Correlation matrix.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Distance matrix.\n",
    "    \"\"\"\n",
    "    return np.sqrt(0.5 * (1 - corr_matrix))\n",
    "\n",
    "\n",
    "def quasi_diagonalize(link: np.ndarray) -> List[int]:\n",
    "    \"\"\"\n",
    "    Quasi-diagonalize the covariance matrix by reordering according to\n",
    "    hierarchical clustering dendrogram.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    link : np.ndarray\n",
    "        Linkage matrix from hierarchical clustering.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[int]\n",
    "        Sorted list of original indices.\n",
    "    \"\"\"\n",
    "    return list(leaves_list(link))\n",
    "\n",
    "\n",
    "def cluster_variance(\n",
    "    cov_matrix: np.ndarray,\n",
    "    cluster_items: List[int]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute variance of an inverse-variance weighted cluster.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cov_matrix : np.ndarray\n",
    "        Full covariance matrix.\n",
    "    cluster_items : List[int]\n",
    "        Indices of items in the cluster.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cluster variance.\n",
    "    \"\"\"\n",
    "    # Extract sub-covariance matrix\n",
    "    sub_cov = cov_matrix[np.ix_(cluster_items, cluster_items)]\n",
    "    \n",
    "    # Inverse-variance weights within cluster\n",
    "    ivp = 1.0 / np.diag(sub_cov)\n",
    "    ivp /= ivp.sum()\n",
    "    \n",
    "    # Cluster variance\n",
    "    return float(ivp @ sub_cov @ ivp)\n",
    "\n",
    "\n",
    "def recursive_bisection(\n",
    "    cov_matrix: np.ndarray,\n",
    "    sorted_indices: List[int]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Recursive bisection for HRP allocation.\n",
    "    \n",
    "    Recursively splits the sorted asset list and allocates weights\n",
    "    based on inverse cluster variance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cov_matrix : np.ndarray\n",
    "        Covariance matrix.\n",
    "    sorted_indices : List[int]\n",
    "        Quasi-diagonalized asset indices.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        HRP weights for each asset.\n",
    "    \"\"\"\n",
    "    n = cov_matrix.shape[0]\n",
    "    weights = np.ones(n)\n",
    "    cluster_items = [sorted_indices]\n",
    "    \n",
    "    while len(cluster_items) > 0:\n",
    "        # Split each cluster into two sub-clusters\n",
    "        new_clusters = []\n",
    "        for cluster in cluster_items:\n",
    "            if len(cluster) > 1:\n",
    "                # Split in half\n",
    "                mid = len(cluster) // 2\n",
    "                left = cluster[:mid]\n",
    "                right = cluster[mid:]\n",
    "                \n",
    "                # Compute cluster variances\n",
    "                var_left = cluster_variance(cov_matrix, left)\n",
    "                var_right = cluster_variance(cov_matrix, right)\n",
    "                \n",
    "                # Allocation factor (inverse variance weighted)\n",
    "                alpha = 1 - var_left / (var_left + var_right)\n",
    "                \n",
    "                # Scale weights\n",
    "                for i in left:\n",
    "                    weights[i] *= alpha\n",
    "                for i in right:\n",
    "                    weights[i] *= (1 - alpha)\n",
    "                \n",
    "                new_clusters.append(left)\n",
    "                new_clusters.append(right)\n",
    "        \n",
    "        cluster_items = [c for c in new_clusters if len(c) > 1]\n",
    "    \n",
    "    return weights / weights.sum()\n",
    "\n",
    "\n",
    "def hierarchical_risk_parity(\n",
    "    returns_df: pd.DataFrame,\n",
    "    linkage_method: str = 'single'\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Compute Hierarchical Risk Parity (HRP) portfolio.\n",
    "    \n",
    "    HRP uses machine learning clustering to build portfolios that:\n",
    "    - Don't require covariance matrix inversion\n",
    "    - Don't need expected return estimates\n",
    "    - Are more stable and diversified\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns_df : pd.DataFrame\n",
    "        Returns DataFrame.\n",
    "    linkage_method : str\n",
    "        Hierarchical clustering linkage method.\n",
    "        Options: 'single', 'complete', 'average', 'ward'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        Portfolio results including weights, clustering info.\n",
    "    \"\"\"\n",
    "    returns = returns_df.values\n",
    "    n = returns.shape[1]\n",
    "    asset_names = list(returns_df.columns)\n",
    "    \n",
    "    # Compute correlation and covariance matrices\n",
    "    cov_matrix = np.cov(returns, rowvar=False)\n",
    "    std = np.sqrt(np.diag(cov_matrix))\n",
    "    corr_matrix = cov_matrix / np.outer(std, std)\n",
    "    np.fill_diagonal(corr_matrix, 1.0)\n",
    "    \n",
    "    # Convert correlation to distance\n",
    "    dist_matrix = correlation_distance(corr_matrix)\n",
    "    \n",
    "    # Hierarchical clustering\n",
    "    # Convert distance matrix to condensed form for linkage\n",
    "    condensed_dist = squareform(dist_matrix, checks=False)\n",
    "    link = linkage(condensed_dist, method=linkage_method)\n",
    "    \n",
    "    # Quasi-diagonalize (get sorted order)\n",
    "    sorted_indices = quasi_diagonalize(link)\n",
    "    \n",
    "    # Recursive bisection for weight allocation\n",
    "    weights = recursive_bisection(cov_matrix, sorted_indices)\n",
    "    \n",
    "    # Portfolio metrics\n",
    "    port_returns = returns @ weights\n",
    "    port_return = np.mean(port_returns)\n",
    "    port_risk = np.std(port_returns)\n",
    "    \n",
    "    return {\n",
    "        'weights': weights,\n",
    "        'asset_names': asset_names,\n",
    "        'return': port_return,\n",
    "        'risk': port_risk,\n",
    "        'sharpe': port_return / port_risk if port_risk > 0 else 0,\n",
    "        'sorted_indices': sorted_indices,\n",
    "        'linkage_matrix': link,\n",
    "        'correlation_matrix': corr_matrix,\n",
    "        'distance_matrix': dist_matrix\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_hrp_dendrogram(\n",
    "    hrp_result: Dict,\n",
    "    title: str = 'HRP Clustering Dendrogram'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot the hierarchical clustering dendrogram.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hrp_result : Dict\n",
    "        Result from hierarchical_risk_parity().\n",
    "    title : str\n",
    "        Plot title.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    dendrogram(\n",
    "        hrp_result['linkage_matrix'],\n",
    "        labels=hrp_result['asset_names'],\n",
    "        leaf_rotation=45\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Assets')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Hierarchical Risk Parity (HRP) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Walk-Forward Optimization (Rolling Backtest)\n",
    "\n",
    "### The Problem: In-Sample Overfitting\n",
    "\n",
    "Computing optimal weights over the entire dataset means the model has \"future knowledge\" of how stocks perform. This leads to overfitting and unrealistic expectations.\n",
    "\n",
    "### The Solution: Walk-Forward Optimization\n",
    "\n",
    "Use a rolling window approach:\n",
    "1. Train on historical data (e.g., 2018-2020)\n",
    "2. Test on out-of-sample data (e.g., 2021)\n",
    "3. Roll the window forward and repeat\n",
    "\n",
    "This is the **only way** to validate a portfolio strategy's true predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# WALK-FORWARD OPTIMIZATION (ROLLING BACKTEST)\n",
    "# ============================================================\n",
    "\n",
    "def walk_forward_backtest(\n",
    "    price_df: pd.DataFrame,\n",
    "    optimizer_func,\n",
    "    train_window: int = 252,\n",
    "    test_window: int = 21,\n",
    "    rebalance_freq: int = 21,\n",
    "    risk_free_rate: float = 0.0,\n",
    "    verbose: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Perform walk-forward (out-of-sample) backtesting.\n",
    "    \n",
    "    This function rolls through time, using past data to compute\n",
    "    weights and then testing on future data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    price_df : pd.DataFrame\n",
    "        Price DataFrame with 'date' column and stock price columns.\n",
    "    optimizer_func : callable\n",
    "        Function that takes returns_df and returns a dict with 'weights'.\n",
    "    train_window : int\n",
    "        Number of trading days for training.\n",
    "    test_window : int\n",
    "        Number of trading days for testing.\n",
    "    rebalance_freq : int\n",
    "        Days between rebalancing.\n",
    "    risk_free_rate : float\n",
    "        Daily risk-free rate.\n",
    "    verbose : bool\n",
    "        Print progress information.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        Backtest results including returns, metrics, weights history.\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    df = price_df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    price_cols = [c for c in df.columns if c != 'date']\n",
    "    prices = df[price_cols].values\n",
    "    dates = df['date'].values\n",
    "    \n",
    "    # Compute returns\n",
    "    returns = prices[1:] / prices[:-1] - 1\n",
    "    return_dates = dates[1:]\n",
    "    \n",
    "    T = len(returns)\n",
    "    n_assets = len(price_cols)\n",
    "    \n",
    "    # Results storage\n",
    "    portfolio_returns = []\n",
    "    portfolio_dates = []\n",
    "    weights_history = []\n",
    "    rebalance_dates = []\n",
    "    \n",
    "    # Walk forward\n",
    "    current_weights = np.ones(n_assets) / n_assets  # Start equal weighted\n",
    "    last_rebalance = 0\n",
    "    \n",
    "    start_idx = train_window\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Walk-Forward Backtest: {len(price_cols)} assets\")\n",
    "        print(f\"Train window: {train_window} days, Test window: {test_window} days\")\n",
    "        print(f\"Rebalance frequency: {rebalance_freq} days\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    for t in range(start_idx, T):\n",
    "        # Check if we need to rebalance\n",
    "        if t - last_rebalance >= rebalance_freq or t == start_idx:\n",
    "            # Use training window to compute weights\n",
    "            train_start = max(0, t - train_window)\n",
    "            train_returns = returns[train_start:t]\n",
    "            \n",
    "            # Create DataFrame for optimizer\n",
    "            train_df = pd.DataFrame(train_returns, columns=price_cols)\n",
    "            \n",
    "            try:\n",
    "                result = optimizer_func(train_df)\n",
    "                if isinstance(result, dict) and 'weights' in result:\n",
    "                    current_weights = result['weights']\n",
    "                elif isinstance(result, dict):\n",
    "                    # Handle nested results (e.g., {'mrr': {'weights': ...}})\n",
    "                    for key in ['mrr', 'gmvp', 'portfolio']:\n",
    "                        if key in result and 'weights' in result[key]:\n",
    "                            current_weights = result[key]['weights']\n",
    "                            break\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Optimization failed at {return_dates[t]}: {e}\")\n",
    "                # Keep previous weights\n",
    "            \n",
    "            last_rebalance = t\n",
    "            rebalance_dates.append(return_dates[t])\n",
    "            weights_history.append(current_weights.copy())\n",
    "            \n",
    "            if verbose and len(rebalance_dates) % 10 == 1:\n",
    "                print(f\"Rebalanced at {return_dates[t]}\")\n",
    "        \n",
    "        # Compute portfolio return for day t\n",
    "        day_return = returns[t] @ current_weights\n",
    "        portfolio_returns.append(day_return)\n",
    "        portfolio_dates.append(return_dates[t])\n",
    "    \n",
    "    # Convert to arrays\n",
    "    portfolio_returns = np.array(portfolio_returns)\n",
    "    \n",
    "    # Compute metrics\n",
    "    cumulative_return = np.prod(1 + portfolio_returns) - 1\n",
    "    annual_return = (1 + cumulative_return) ** (252 / len(portfolio_returns)) - 1\n",
    "    annual_volatility = np.std(portfolio_returns) * np.sqrt(252)\n",
    "    sharpe_ratio = (np.mean(portfolio_returns) - risk_free_rate) / np.std(portfolio_returns) * np.sqrt(252)\n",
    "    \n",
    "    # Maximum drawdown\n",
    "    cumulative = np.cumprod(1 + portfolio_returns)\n",
    "    running_max = np.maximum.accumulate(cumulative)\n",
    "    drawdowns = (cumulative - running_max) / running_max\n",
    "    max_drawdown = np.min(drawdowns)\n",
    "    \n",
    "    # Sortino ratio\n",
    "    downside_returns = np.minimum(portfolio_returns, 0)\n",
    "    downside_std = np.sqrt(np.mean(downside_returns ** 2)) * np.sqrt(252)\n",
    "    sortino_ratio = annual_return / downside_std if downside_std > 0 else 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"-\" * 50)\n",
    "        print(\"\\nBacktest Results:\")\n",
    "        print(f\"  Cumulative Return: {cumulative_return * 100:.2f}%\")\n",
    "        print(f\"  Annual Return: {annual_return * 100:.2f}%\")\n",
    "        print(f\"  Annual Volatility: {annual_volatility * 100:.2f}%\")\n",
    "        print(f\"  Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
    "        print(f\"  Sortino Ratio: {sortino_ratio:.4f}\")\n",
    "        print(f\"  Max Drawdown: {max_drawdown * 100:.2f}%\")\n",
    "        print(f\"  Number of Rebalances: {len(rebalance_dates)}\")\n",
    "    \n",
    "    return {\n",
    "        'returns': portfolio_returns,\n",
    "        'dates': portfolio_dates,\n",
    "        'cumulative_return': cumulative_return,\n",
    "        'annual_return': annual_return,\n",
    "        'annual_volatility': annual_volatility,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'sortino_ratio': sortino_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'weights_history': weights_history,\n",
    "        'rebalance_dates': rebalance_dates\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_strategies(\n",
    "    price_df: pd.DataFrame,\n",
    "    strategies: Dict[str, callable],\n",
    "    train_window: int = 252,\n",
    "    rebalance_freq: int = 21,\n",
    "    risk_free_rate: float = 0.0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare multiple portfolio strategies using walk-forward testing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    price_df : pd.DataFrame\n",
    "        Price DataFrame.\n",
    "    strategies : Dict[str, callable]\n",
    "        Dictionary mapping strategy names to optimizer functions.\n",
    "    train_window : int\n",
    "        Training window in days.\n",
    "    rebalance_freq : int\n",
    "        Rebalancing frequency in days.\n",
    "    risk_free_rate : float\n",
    "        Daily risk-free rate.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Comparison metrics for all strategies.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, optimizer in strategies.items():\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Testing Strategy: {name}\")\n",
    "        print('=' * 60)\n",
    "        \n",
    "        result = walk_forward_backtest(\n",
    "            price_df,\n",
    "            optimizer,\n",
    "            train_window=train_window,\n",
    "            rebalance_freq=rebalance_freq,\n",
    "            risk_free_rate=risk_free_rate,\n",
    "            verbose=True\n",
    "        )\n",
    "        results[name] = result\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison = pd.DataFrame({\n",
    "        name: {\n",
    "            'Cumulative Return (%)': res['cumulative_return'] * 100,\n",
    "            'Annual Return (%)': res['annual_return'] * 100,\n",
    "            'Annual Volatility (%)': res['annual_volatility'] * 100,\n",
    "            'Sharpe Ratio': res['sharpe_ratio'],\n",
    "            'Sortino Ratio': res['sortino_ratio'],\n",
    "            'Max Drawdown (%)': res['max_drawdown'] * 100\n",
    "        }\n",
    "        for name, res in results.items()\n",
    "    }).T\n",
    "    \n",
    "    return comparison, results\n",
    "\n",
    "\n",
    "def plot_backtest_results(\n",
    "    results: Dict[str, Dict],\n",
    "    title: str = 'Strategy Comparison'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot cumulative returns for multiple strategies.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    results : Dict[str, Dict]\n",
    "        Dictionary of strategy results from walk_forward_backtest.\n",
    "    title : str\n",
    "        Plot title.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for name, res in results.items():\n",
    "        cumulative = np.cumprod(1 + res['returns'])\n",
    "        plt.plot(res['dates'], cumulative, label=name, linewidth=1.5)\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Walk-Forward Optimization (Rolling Backtest) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualization Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION UTILITIES\n",
    "# ============================================================\n",
    "\n",
    "def plot_portfolio_weights(\n",
    "    weights: np.ndarray,\n",
    "    asset_names: List[str],\n",
    "    title: str = 'Portfolio Weights'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot portfolio weights as a bar chart.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    weights : np.ndarray\n",
    "        Portfolio weights.\n",
    "    asset_names : List[str]\n",
    "        Asset names.\n",
    "    title : str\n",
    "        Plot title.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(max(8, len(asset_names) * 0.5), 5))\n",
    "    x = np.arange(len(asset_names))\n",
    "    colors = ['steelblue' if w >= 0 else 'red' for w in weights]\n",
    "    plt.bar(x, weights * 100, color=colors)\n",
    "    plt.xticks(x, asset_names, rotation=45, ha='right')\n",
    "    plt.ylabel('Weight (%)')\n",
    "    plt.title(title)\n",
    "    plt.axhline(0, color='black', linewidth=0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_efficient_frontier_comparison(\n",
    "    returns_df: pd.DataFrame,\n",
    "    portfolios: Dict[str, Dict],\n",
    "    n_random: int = 2000,\n",
    "    title: str = 'Efficient Frontier with Optimized Portfolios'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot efficient frontier with multiple portfolio strategies.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns_df : pd.DataFrame\n",
    "        Returns DataFrame.\n",
    "    portfolios : Dict[str, Dict]\n",
    "        Dictionary mapping portfolio names to results with 'return' and 'risk'.\n",
    "    n_random : int\n",
    "        Number of random portfolios to plot.\n",
    "    title : str\n",
    "        Plot title.\n",
    "    \"\"\"\n",
    "    returns = returns_df.values\n",
    "    n = returns.shape[1]\n",
    "    mu = np.mean(returns, axis=0)\n",
    "    Sigma = np.cov(returns, rowvar=False)\n",
    "    \n",
    "    # Generate random portfolios\n",
    "    random_returns = []\n",
    "    random_risks = []\n",
    "    for _ in range(n_random):\n",
    "        w = np.random.dirichlet(np.ones(n))\n",
    "        r = w @ mu\n",
    "        risk = np.sqrt(w @ Sigma @ w)\n",
    "        random_returns.append(r)\n",
    "        random_risks.append(risk)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot random portfolios\n",
    "    plt.scatter(\n",
    "        np.array(random_risks) * np.sqrt(TRADING_DAYS) * 100,\n",
    "        np.array(random_returns) * TRADING_DAYS * 100,\n",
    "        c='lightgray', alpha=0.3, s=10, label='Random'\n",
    "    )\n",
    "    \n",
    "    # Plot optimized portfolios\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
    "    markers = ['o', 's', '^', 'D', 'v', 'p']\n",
    "    \n",
    "    for i, (name, port) in enumerate(portfolios.items()):\n",
    "        if 'return' in port and 'risk' in port:\n",
    "            plt.scatter(\n",
    "                port['risk'] * np.sqrt(TRADING_DAYS) * 100,\n",
    "                port['return'] * TRADING_DAYS * 100,\n",
    "                c=colors[i % len(colors)],\n",
    "                marker=markers[i % len(markers)],\n",
    "                s=150, edgecolors='black', linewidth=1.5,\n",
    "                label=name, zorder=5\n",
    "            )\n",
    "    \n",
    "    plt.xlabel('Annual Risk (%)')\n",
    "    plt.ylabel('Annual Return (%)')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_portfolio_summary(name: str, result: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Print a formatted portfolio summary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Portfolio name.\n",
    "    result : Dict\n",
    "        Portfolio result dictionary.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"{name}\")\n",
    "    print('=' * 60)\n",
    "    \n",
    "    if 'weights' in result:\n",
    "        weights = result['weights']\n",
    "        if 'asset_names' in result:\n",
    "            print(\"\\nPortfolio Weights:\")\n",
    "            for name, w in zip(result['asset_names'], weights):\n",
    "                if abs(w) > 0.001:\n",
    "                    print(f\"  {name:15s}: {w * 100:7.3f}%\")\n",
    "        else:\n",
    "            print(f\"\\nWeights: {weights}\")\n",
    "    \n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    if 'return' in result:\n",
    "        print(f\"  Daily Return:   {result['return'] * 100:.4f}%\")\n",
    "        print(f\"  Annual Return:  {result['return'] * TRADING_DAYS * 100:.2f}%\")\n",
    "    if 'risk' in result:\n",
    "        print(f\"  Daily Risk:     {result['risk'] * 100:.4f}%\")\n",
    "        print(f\"  Annual Risk:    {result['risk'] * np.sqrt(TRADING_DAYS) * 100:.2f}%\")\n",
    "    if 'sharpe' in result:\n",
    "        print(f\"  Sharpe Ratio:   {result['sharpe'] * np.sqrt(TRADING_DAYS):.4f}\")\n",
    "    if 'sortino_ratio' in result:\n",
    "        print(f\"  Sortino Ratio:  {result['sortino_ratio']:.4f}\")\n",
    "    if 'cvar' in result:\n",
    "        print(f\"  CVaR ({result.get('confidence', 0.95)*100:.0f}%):    {result['cvar'] * 100:.4f}%\")\n",
    "    if 'shrinkage_intensity' in result:\n",
    "        print(f\"  Shrinkage Int.: {result['shrinkage_intensity']:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Visualization utilities defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example Usage and Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXAMPLE: GENERATE SAMPLE DATA\n",
    "# ============================================================\n",
    "\n",
    "# Generate synthetic data for demonstration\n",
    "# (Replace this with real data loading for actual analysis)\n",
    "\n",
    "print(\"Generating sample data for demonstration...\")\n",
    "sample_data = generate_sample_data(n_assets=10, n_days=500, seed=42)\n",
    "print(f\"Sample data shape: {sample_data.shape}\")\n",
    "print(f\"Date range: {sample_data['date'].min()} to {sample_data['date'].max()}\")\n",
    "print(f\"Assets: {[c for c in sample_data.columns if c != 'date']}\")\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXAMPLE: COMPARE ALL OPTIMIZATION METHODS\n",
    "# ============================================================\n",
    "\n",
    "# Compute returns\n",
    "returns_df = compute_returns(sample_data)\n",
    "print(f\"Returns shape: {returns_df.shape}\")\n",
    "\n",
    "# 1. Black-Litterman Portfolio (with sample views)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. BLACK-LITTERMAN MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example views: STOCK_01 will outperform, STOCK_02 will underperform STOCK_03\n",
    "views = [\n",
    "    {'assets': 'STOCK_01', 'return': 0.0005, 'type': 'absolute', 'confidence': 0.8},\n",
    "    {'assets': ['STOCK_02', 'STOCK_03'], 'return': 0.0002, 'type': 'relative', 'confidence': 0.6}\n",
    "]\n",
    "\n",
    "bl_result = black_litterman_portfolio(\n",
    "    returns_df,\n",
    "    views=views,\n",
    "    risk_aversion=2.5,\n",
    "    tau=0.05\n",
    ")\n",
    "print_portfolio_summary(\"Black-Litterman Portfolio\", bl_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Ledoit-Wolf Shrinkage\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. LEDOIT-WOLF SHRINKAGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lw_result = portfolio_optimizer_shrinkage(\n",
    "    returns_df,\n",
    "    risk_free_rate=RISK_FREE_RATE,\n",
    "    shrinkage_target='constant_correlation'\n",
    ")\n",
    "print(f\"\\nShrinkage Intensity: {lw_result['shrinkage_intensity']:.4f}\")\n",
    "print(f\"(0 = pure sample cov, 1 = pure structured target)\")\n",
    "\n",
    "print(\"\\nGMVP Portfolio (Ledoit-Wolf):\")\n",
    "print(f\"  Return: {lw_result['gmvp']['return'] * TRADING_DAYS * 100:.2f}%\")\n",
    "print(f\"  Risk: {lw_result['gmvp']['risk'] * np.sqrt(TRADING_DAYS) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nMax Sharpe Portfolio (Ledoit-Wolf):\")\n",
    "print(f\"  Return: {lw_result['mrr']['return'] * TRADING_DAYS * 100:.2f}%\")\n",
    "print(f\"  Risk: {lw_result['mrr']['risk'] * np.sqrt(TRADING_DAYS) * 100:.2f}%\")\n",
    "print(f\"  Sharpe: {lw_result['mrr']['sharpe'] * np.sqrt(TRADING_DAYS):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. EWMA Dynamic Volatility\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. EWMA DYNAMIC VOLATILITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ewma_result = portfolio_optimizer_ewma(\n",
    "    returns_df,\n",
    "    risk_free_rate=RISK_FREE_RATE,\n",
    "    lambda_decay=0.94\n",
    ")\n",
    "\n",
    "print(\"\\nGMVP Portfolio (EWMA):\")\n",
    "print(f\"  Return: {ewma_result['gmvp']['return'] * TRADING_DAYS * 100:.2f}%\")\n",
    "print(f\"  Risk: {ewma_result['gmvp']['risk'] * np.sqrt(TRADING_DAYS) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nMax Sharpe Portfolio (EWMA):\")\n",
    "print(f\"  Return: {ewma_result['mrr']['return'] * TRADING_DAYS * 100:.2f}%\")\n",
    "print(f\"  Risk: {ewma_result['mrr']['risk'] * np.sqrt(TRADING_DAYS) * 100:.2f}%\")\n",
    "print(f\"  Sharpe: {ewma_result['mrr']['sharpe'] * np.sqrt(TRADING_DAYS):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Downside Risk Optimization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. DOWNSIDE RISK OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Min CVaR Portfolio\n",
    "cvar_result = min_cvar_portfolio(returns_df, confidence=0.95)\n",
    "print(\"\\nMinimum CVaR Portfolio:\")\n",
    "print(f\"  Return: {cvar_result['return'] * TRADING_DAYS * 100:.2f}%\")\n",
    "print(f\"  Risk: {cvar_result['risk'] * np.sqrt(TRADING_DAYS) * 100:.2f}%\")\n",
    "print(f\"  VaR (95%): {cvar_result['var'] * 100:.4f}%\")\n",
    "print(f\"  CVaR (95%): {cvar_result['cvar'] * 100:.4f}%\")\n",
    "\n",
    "# Max Sortino Portfolio\n",
    "sortino_result = max_sortino_portfolio(returns_df, risk_free_rate=RISK_FREE_RATE)\n",
    "print(\"\\nMaximum Sortino Portfolio:\")\n",
    "print(f\"  Return: {sortino_result['return'] * TRADING_DAYS * 100:.2f}%\")\n",
    "print(f\"  Risk: {sortino_result['risk'] * np.sqrt(TRADING_DAYS) * 100:.2f}%\")\n",
    "print(f\"  Sharpe Ratio: {sortino_result['sharpe_ratio'] * np.sqrt(TRADING_DAYS):.4f}\")\n",
    "print(f\"  Sortino Ratio: {sortino_result['sortino_ratio'] * np.sqrt(TRADING_DAYS):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Hierarchical Risk Parity\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5. HIERARCHICAL RISK PARITY (HRP)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "hrp_result = hierarchical_risk_parity(returns_df, linkage_method='single')\n",
    "print_portfolio_summary(\"HRP Portfolio\", hrp_result)\n",
    "\n",
    "# Plot dendrogram\n",
    "print(\"\\nHRP Clustering Dendrogram:\")\n",
    "plot_hrp_dendrogram(hrp_result, title='HRP Asset Clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPARE ALL METHODS ON EFFICIENT FRONTIER\n",
    "# ============================================================\n",
    "\n",
    "# Collect all portfolio results\n",
    "all_portfolios = {\n",
    "    'Black-Litterman': bl_result,\n",
    "    'Ledoit-Wolf MRR': lw_result['mrr'],\n",
    "    'EWMA MRR': ewma_result['mrr'],\n",
    "    'Min CVaR': cvar_result,\n",
    "    'Max Sortino': sortino_result,\n",
    "    'HRP': hrp_result\n",
    "}\n",
    "\n",
    "plot_efficient_frontier_comparison(\n",
    "    returns_df,\n",
    "    all_portfolios,\n",
    "    n_random=3000,\n",
    "    title='Efficient Frontier: Comparison of Advanced Optimization Methods'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. WALK-FORWARD BACKTESTING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"6. WALK-FORWARD BACKTESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define strategies to compare\n",
    "def equal_weight_optimizer(returns_df):\n",
    "    n = returns_df.shape[1]\n",
    "    return {'weights': np.ones(n) / n}\n",
    "\n",
    "def hrp_optimizer(returns_df):\n",
    "    return hierarchical_risk_parity(returns_df)\n",
    "\n",
    "def lw_optimizer(returns_df):\n",
    "    result = portfolio_optimizer_shrinkage(returns_df, shrinkage_target='constant_correlation')\n",
    "    return result['mrr']\n",
    "\n",
    "def sortino_optimizer(returns_df):\n",
    "    return max_sortino_portfolio(returns_df)\n",
    "\n",
    "strategies = {\n",
    "    'Equal Weight': equal_weight_optimizer,\n",
    "    'HRP': hrp_optimizer,\n",
    "    'Ledoit-Wolf': lw_optimizer,\n",
    "    'Max Sortino': sortino_optimizer\n",
    "}\n",
    "\n",
    "# Run comparison\n",
    "comparison_df, backtest_results = compare_strategies(\n",
    "    sample_data,\n",
    "    strategies,\n",
    "    train_window=126,  # 6 months\n",
    "    rebalance_freq=21,  # Monthly\n",
    "    risk_free_rate=RISK_FREE_RATE\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STRATEGY COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative returns comparison\n",
    "plot_backtest_results(\n",
    "    backtest_results,\n",
    "    title='Walk-Forward Backtest: Strategy Comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook implements six advanced portfolio optimization techniques that address the limitations of classical Mean-Variance Optimization:\n",
    "\n",
    "| Problem | Classical MVO Limitation | Solution Implemented |\n",
    "|---------|-------------------------|---------------------|\n",
    "| Estimation Error | Uses historical means blindly | **Black-Litterman Model** |\n",
    "| Covariance Instability | Sample covariance is noisy | **Ledoit-Wolf Shrinkage** |\n",
    "| Static Risk | Assumes constant volatility | **EWMA Dynamic Volatility** |\n",
    "| Normal Assumptions | Ignores fat tails/skewness | **CVaR/Sortino Optimization** |\n",
    "| Matrix Inversion | Numerical instability | **Hierarchical Risk Parity** |\n",
    "| In-Sample Bias | Overfits to historical data | **Walk-Forward Backtesting** |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Black-Litterman** provides more intuitive and stable weights by starting from market equilibrium\n",
    "2. **Shrinkage estimators** reduce the impact of spurious correlations\n",
    "3. **EWMA** gives more weight to recent market conditions\n",
    "4. **CVaR/Sortino** focus on actual downside risk, not symmetric variance\n",
    "5. **HRP** uses machine learning clustering and doesn't require matrix inversion\n",
    "6. **Walk-forward testing** is essential for validating any portfolio strategy\n",
    "\n",
    "### Usage with Real Data\n",
    "\n",
    "To use these methods with real stock data:\n",
    "1. Load data using `read_stock_data_from_folder()` or your own data source\n",
    "2. Filter to desired date range and stocks using `filter_dataset()`\n",
    "3. Apply any of the optimization methods\n",
    "4. Validate with `walk_forward_backtest()` before deploying"
   ]
  }
 ]
}